{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"COMP2825 - Computer Architecture and Organization","text":""},{"location":"#overview","title":"Overview","text":"<p>For full course information COMP 2825 - BCIT Course taken in Fall 2025 term (outline) with Rahim Oraji.</p> <ul> <li>Midterm and Final in-person</li> <li>weekly quizes</li> <li>weekly labs<ul> <li>each lab will involve two things:<ul> <li>watching a video and writing a one page summary (what is this, high school?)</li> <li>answering questions based on the slides</li> <li>only given 5 days for the labs because reasons</li> </ul> </li> </ul> </li> </ul>"},{"location":"wk_one/","title":"Week 1","text":""},{"location":"wk_one/#computer-architecture","title":"Computer Architecture","text":"<p>Computer Architecture: The study of how to design the parts of a computer  system visible to programmers, including the instruction set, memory hierarchy, and execution model (e.g., pipelined execution).</p> <p>Computer organization: Focuses on the internal structure of hardware  components like ALUs, registers, and buses.</p> <p>In practice, both used interchangably.</p> <p>Studying computer architecture helps us understand:</p> <ul> <li>code implementation,</li> <li>performance optimization,</li> <li>concepts such as parallelism, caching, and pipelining,</li> <li>tradeoffs between hardware and software design choices</li> </ul>"},{"location":"wk_one/#computers-and-programs","title":"Computers and Programs","text":"<p>A computer executes given instructions (programs) to produce results for  the user.</p>"},{"location":"wk_one/#program-execution","title":"Program Execution","text":"<p>In general, when it comes to executing a given program, there are two methods:</p> <ol> <li>Translation (compilation)<ul> <li>Converts a high-level program into a low-level (machine code) version  before execution</li> </ul> </li> <li>Interpretation<ul> <li>A low-level machine program (the interpreter) reads and executes  high-level programs instruction by instruction during runtime. </li> </ul> </li> </ol>"},{"location":"wk_one/#program-languages-and-levels","title":"Program Languages and Levels","text":"<p>A program is a set of instructions written in a high level programming language.</p> <ol> <li> <p>Machine Language (low level) Raw binary code (zeros and ones) executed directly by the CPU.  </p> </li> <li> <p>Human-Readable Machine Language Uses mnemonics instead of binary, but still closely tied to hardware.      Each instruction maps directly to machine code.  Example: Assembly (1949)</p> </li> <li> <p>\"Modern\" Programming Languages (high level) Abstract, human-readable languages designed to be portable and easier to write.     Compiled or interpreted into lower-level code.  Examples:  C (1970),  Java (1995), Python (1991), JavaScript (1995) </p> </li> </ol>"},{"location":"wk_one/#multilevel-machines","title":"Multilevel Machines","text":"<ol> <li>Digital Logic Level </li> <li>Built from basic logic gates (AND, OR, NOT).  </li> <li> <p>These gates form circuits that make up components like CPU and memory.  </p> </li> <li> <p>Microarchitecture Level </p> </li> <li>Executes instructions using hardware components: ALU, registers, microprograms, memory.  </li> <li>A microprogram controls how instructions are executed.  </li> <li> <p>Represents the actual organization of the CPU at the circuit level.  </p> </li> <li> <p>Instruction Set Architecture (ISA) Level </p> </li> <li>Defines the set of machine instructions available (e.g., <code>LOAD</code>, <code>ADD</code>, <code>STORE</code>).  </li> <li>Independent of how the CPU actually implements them.  </li> <li> <p>Analogy: Like a menu in a restaurant \u2014 lists the \u201cdishes\u201d (operations) you can order, but not how they\u2019re cooked.  </p> </li> <li> <p>Operating System Machine Level </p> </li> <li>Main duties: memory management, process execution, and system resource protection.  </li> <li>Provides abstraction to simplify programming.  </li> <li>Assembly instructions generally pass through this level unchanged.  </li> <li> <p>Some interpretation happens here, as the OS manages low-level execution details.  </p> </li> <li> <p>Assembly Language Level </p> </li> <li>Human-readable mnemonics (e.g., <code>MOV</code>, <code>ADD</code>).  </li> <li>Harder to use \u2014 even simple programs require many lines.  </li> <li> <p>Translated into machine code by an assembler.  </p> </li> <li> <p>Problem-Oriented Language Level (High-Level Languages) </p> </li> <li>Includes languages like C, Java, Python.  </li> <li>Easier for humans to read, write, and maintain.  </li> <li>Compilers or interpreters convert these programs into assembly/machine code.  </li> </ol>"},{"location":"wk_one/#hardware-vs-software-and-control-units","title":"Hardware vs Software and Control Units","text":"<p>Anything that can be done with software can also be done with hardware, and vice versa (functionality).</p> <p>Hardware is generally faster and more expensive.</p>"},{"location":"wk_one/#control-unit-microprogrammed-vs-hardwired","title":"Control Unit: Microprogrammed vs Hardwired","text":"<p>The Control Unit (CU) generates the control signals that tell the CPU\u2019s datapath (ALU, registers, memory, etc.) what to do when executing instructions.</p>"},{"location":"wk_one/#microprogrammed-control-unit","title":"Microprogrammed Control Unit","text":"<ul> <li>Software-based control: Control signals are stored in a special memory (control memory, often ROM).  </li> <li>A microprogram is a sequence of microinstructions that generate the necessary control signals.  </li> <li>Execution = fetching and running these microinstructions in sequence.  </li> <li>Advantages:</li> <li>Easy to modify or extend (new instructions can be added by changing the microprogram).  </li> <li>Flexible and easier to design.  </li> <li>Disadvantages:</li> <li>Slower than hardwired control, since every instruction is broken into microinstructions that must be fetched and interpreted.  </li> </ul> <p>Example (slides): - Microprogram (P1): LEDs moving to the right. - Microprogram (P2): LEDs moving to the left. - Stored in ROM, and switching the program changes the behavior.</p>"},{"location":"wk_one/#hardwired-control-unit","title":"Hardwired Control Unit","text":"<ul> <li>Hardware-based control: Control signals are generated directly by fixed logic circuits.  </li> <li>No control memory \u2014 logic gates are wired to decode the opcode and trigger control signals.  </li> <li>Advantages:</li> <li>Much faster (signals generated directly by circuits).  </li> <li>Disadvantages:</li> <li>Inflexible \u2014 changing the instruction set requires redesigning the hardware.  </li> <li>More complex design as instruction sets grow.  </li> </ul>"},{"location":"wk_one/#comparison-table","title":"Comparison Table","text":"Aspect Microprogrammed CU Hardwired CU Implementation Control memory + microinstructions Fixed logic circuits Flexibility Easy to modify, new instructions easy Difficult to change Speed Slower (extra step: microinstruction) Faster (direct hardware) Complexity Simpler design More complex for large ISAs"},{"location":"wk_one/#historical-context","title":"Historical Context","text":"<ul> <li>Early computers: Direct hardwired execution.  </li> <li>Mid-generation: Microprogramming introduced to simplify control of complex instruction sets.  </li> <li>Modern trend: Moving back toward hardwired (or simplified ISAs) because microprogramming slows down execution.  </li> </ul>"},{"location":"wk_one/#computer-architecture_1","title":"Computer Architecture","text":""},{"location":"wk_one/#computer-generations","title":"Computer Generations","text":"<ul> <li>Zeroth Generation (1642 \u2013 1945): Mechanical Computers (e.g., Pascal's calculator).</li> <li>First Generation (1945 \u2013 1955): Vacuum Tubes (e.g., ENIAC \u2013 large,   power-hungry, first electronic general-purpose computer). * Von Neumann   Architecture (1945): Both data and program stored in memory (still the basis   for most digital computers).</li> <li>Second Generation (1955 \u2013 1965): Transistors (e.g., IBM 7094, PDP-8 with a   single bus/omnibus). Smaller, faster, cheaper.</li> <li>Third Generation (1965 \u2013 1980): Integrated Circuits (IC) (e.g., IBM 360).  Dozens of transistors on a single chip, leading to even smaller, faster, and cheaper computers.</li> <li>Fourth Generation (1980 \u2013 ?): Very Large Scale Integration (VLSI) (e.g.,   Intel CPUs). Millions of transistors on a single chip.</li> </ul>"},{"location":"wk_one/#moores-law","title":"Moore's Law","text":"<ul> <li>Gordon Moore (Intel, 1965).  </li> <li>Transistors on a chip roughly double every ~2 years.  </li> <li>Result: smaller, faster, cheaper computers.  </li> <li>Recently slowing due to physical limits.</li> </ul>"},{"location":"wk_one/#harvard-vs-von-neumann-architecture","title":"Harvard vs. Von Neumann Architecture","text":"<ul> <li>Harvard Architecture: <ul> <li>Separate memory for data and code.</li> <li>Two separate data paths for instructions and data. </li> <li>Faster (can access instructions and data simultaneously). </li> <li>Used in microcontrollers, DSPs. </li> <li>More complex design.</li> </ul> </li> <li>Von Neumann Architecture: <ul> <li>Shared memory for data and code. </li> <li>One shared data path for both. </li> <li>Slower (one access at a time). </li> <li>Used in general-purpose computers (PCs, laptops). </li> <li>Simpler design.</li> </ul> </li> </ul>"},{"location":"wk_one/#instruction-set-architectures-isas","title":"Instruction Set Architectures (ISAs)","text":"<p>An Instruction Set Architecture (ISA) defines the set of instructions that a CPU can execute. Different families of processors use different ISAs, optimized for different purposes.</p>"},{"location":"wk_one/#x86-architecture","title":"x86 Architecture","text":"<ul> <li>Originated with Intel 8086 (1978).</li> <li>Complex Instruction Set Computer (CISC) \u2013 many specialized instructions.</li> <li>Dominant in personal computers and servers.</li> <li>High backward compatibility (can still run very old code).</li> <li>Large, complex instruction set with advanced microarchitectural optimizations.</li> </ul>"},{"location":"wk_one/#arm-architecture","title":"ARM Architecture","text":"<ul> <li>First appeared in Acorn Archimedes (1987), later spun off as ARM Holdings.</li> <li>Reduced Instruction Set Computer (RISC) \u2013 small, simple set of instructions.</li> <li>Used in mobile devices, tablets, embedded systems, and increasingly laptops and servers (e.g., Apple M1/M2, AWS Graviton).</li> <li>High performance per watt (very energy efficient).</li> <li>Scales from tiny microcontrollers to high-end processors.</li> </ul>"},{"location":"wk_one/#avr-architecture","title":"AVR Architecture","text":"<ul> <li>Developed in 1996 (Norwegian Institute of Technology, later Atmel).</li> <li>RISC-based 8-bit microcontroller ISA.</li> <li>Widely used in embedded systems and Arduino boards.</li> <li>Small, low-power, inexpensive.</li> <li>Integrates Flash, EEPROM, and RAM on the same chip.</li> </ul>"},{"location":"wk_one/#comparison-table_1","title":"Comparison Table","text":"Feature x86 (CISC) ARM (RISC) AVR (RISC) First Release 1978 (Intel 8086) 1987 (Acorn Archimedes) 1996 (Atmel) Instruction Set Large, complex Small, simple Small, simple (8-bit) Main Usage PCs, laptops, servers Mobile, tablets, servers, IoT Embedded, Arduino Efficiency High performance, less efficient per watt Very energy efficient Extremely low power Cost/Scale Expensive, high complexity Scales across many devices Low cost, hobbyist friendly"},{"location":"wk_three/","title":"Boolean Algebra, Logic Simplification, K-Maps, and Logic Gates","text":"<p>These notes summarize the key concepts needed to understand Boolean expressions, simplification methods, truth tables, Karnaugh maps, and the design and interpretation of logic circuits.</p>"},{"location":"wk_three/#boolean-algebra-basics","title":"Boolean Algebra Basics","text":"<p>Boolean algebra deals with binary values and logic operations.</p> Concept Description Variables A, B, C, ... Values 0 (false), 1 (true) Functions F(A,B,C), G(A,B), ... Operations NOT (A'), AND (A\u00b7B), OR (A+B), XOR (A\u2295B) Examples of Boolean expressions <p>A\u00b7B A + B A'(B + C) A \u2295 B</p>"},{"location":"wk_three/#truth-tables","title":"Truth Tables","text":"<p>A truth table lists all possible input combinations and the resulting output.</p> Example Truth Table for AND A B A\u00b7B 0 0 0 0 1 0 1 0 0 1 1 1"},{"location":"wk_three/#boolean-identities-simplification-rules","title":"Boolean Identities (Simplification Rules)","text":"Rule Identity Complement A + A' = 1, A\u00b7A' = 0 Identity A + 0 = A, A\u00b71 = A Null A + 1 = 1, A\u00b70 = 0 Idempotent A + A = A, A\u00b7A = A De Morgan (A+B)' = A'\u00b7B', (A\u00b7B)' = A' + B' <p>Use these identities to reduce expressions and minimize gate usage.</p>"},{"location":"wk_three/#simplification-example","title":"Simplification Example","text":"<p>Simplify: F = A'B + AB' + AB</p> Step-by-Step Simplification <p>F = A'B + AB' + AB  = B(A' + A) + AB'  = B(1) + AB'  = B + AB'  = (B + A)(B + B')  = A + B</p> <p>Final Answer: F = A + B</p>"},{"location":"wk_three/#canonical-forms","title":"Canonical Forms","text":""},{"location":"wk_three/#sum-of-products-sop","title":"Sum of Products (SOP)","text":"<ul> <li>Use rows in truth table where F = 1</li> <li>Form minterms (AND of input variables)</li> <li>OR them together</li> </ul>"},{"location":"wk_three/#product-of-sums-pos","title":"Product of Sums (POS)","text":"<ul> <li>Use rows in truth table where F = 0</li> <li>Form maxterms (OR of input variables)</li> <li>AND them together</li> </ul>"},{"location":"wk_three/#karnaugh-maps-k-maps","title":"Karnaugh Maps (K-Maps)","text":"<p>K-maps are visual tools for simplifying Boolean expressions.</p>"},{"location":"wk_three/#rules","title":"Rules","text":"<ul> <li>Group 1s in sizes 1, 2, 4, 8, ...</li> <li>Groups must be rectangular</li> <li>Groups may wrap around edges</li> <li>Each 1 must be included in at least one group</li> <li>Larger groups produce simpler expressions</li> </ul>"},{"location":"wk_three/#k-map-example-3-variables-a-b-c","title":"K-Map Example (3 variables: A, B, C)","text":"<p>Truth table function values: F = 1 for m1, m3, m5, m7 (binary inputs 001, 011, 101, 111)</p> <p>K-Map:</p> AB \\ C 0 1 00 0 1 01 0 1 11 0 1 10 0 1 <p>Group the column where C = 1 \u2192 all 1\u2019s in one group.</p> <p>This means C remains; A and B vary \u2192 they drop out.</p> <p>Simplified Expression: F = C</p> Why this simplifies to C <p>C is the only input that is 1 in all grouped minterms. A and B change, so they do not appear in the final expression.</p>"},{"location":"wk_three/#logic-gates","title":"Logic Gates","text":"Gate Symbol Function NOT A' Inverts input AND A\u00b7B Output 1 if both inputs = 1 OR A + B Output 1 if any input = 1 XOR A \u2295 B Output 1 only if inputs differ NAND (A\u00b7B)' Inverted AND NOR (A + B)' Inverted OR"},{"location":"wk_two/","title":"Week 2","text":""},{"location":"wk_two/#data-representation-number-systems","title":"Data Representation &amp; Number Systems","text":"<p>Computers operate using binary (base 2), so we often need to convert between number systems and represent integers and real numbers efficiently.</p>"},{"location":"wk_two/#number-bases","title":"Number Bases","text":"System Base Digits Used Example Decimal 10 0\u20139 245\u2081\u2080 Binary 2 0\u20131 111101\u2082 Octal 8 0\u20137 725\u2088 Hexadecimal 16 0\u20139, A\u2013F A3F\u2081\u2086"},{"location":"wk_two/#conversions-between-bases","title":"Conversions Between Bases","text":""},{"location":"wk_two/#decimal-binary-repeated-division-by-2","title":"Decimal \u2192 Binary (Repeated division by 2)","text":"<ol> <li>Divide by 2.</li> <li>Record remainder.</li> <li>Continue until quotient = 0.</li> <li>Binary result = remainders read bottom \u2192 top.</li> </ol> Example: 224\u2081\u2080 \u2192 Binary <pre><code>224 \u00f7 2 = 112    remainder 0\n112 \u00f7 2 = 56     remainder 0\n56 \u00f7 2 = 28      remainder 0\n28 \u00f7 2 = 14      remainder 0\n14 \u00f7 2 = 7       remainder 0\n7 \u00f7 2 = 3        remainder 1\n3 \u00f7 2 = 1        remainder 1\n1 \u00f7 2 = 0        remainder 1\n\n11100000\u2082   (comes from the remainders)\n</code></pre>"},{"location":"wk_two/#binary-decimal-positional-weighting","title":"Binary \u2192 Decimal (Positional weighting)","text":"<p>(a\u2099 \u2026 a\u2083a\u2082a\u2081a\u2080)\u2082 = \u03a3 a\u1d62 \u00b7 2\u2071</p> Example: 1101\u2082 \u2192 Decimal <p>1101\u2082</p> i\u2083 i\u2082 i\u2081 i\u2080 Bit value a\u1d62 1 1 0 1 Term a\u1d62\u00b72\u2071 1\u00d72\u00b3 1\u00d72\u00b2 0\u00d72\u00b9 1\u00d72\u2070 Value 8 4 0 1 <p>Therefore, 1101\u2082 = 8 + 4 + 0 + 1 = 13\u2081\u2080</p>"},{"location":"wk_two/#binary-hexadecimal-group-by-4-bits","title":"Binary \u2194 Hexadecimal (Group by 4 bits)","text":"Example: 110101101001\u2082 \u2192 Hexadecimal Binary (4-bit groups) 1101 0110 1001 Hexadecimal D 6 9 Example: AF2\u2081\u2086 \u2192 Binary Hexadecimal A F 2 Binary (4-bit groups) 1010 1111 0010 <p>-</p>"},{"location":"wk_two/#representing-integers-in-binary","title":"Representing Integers in Binary","text":""},{"location":"wk_two/#unsigned-integers","title":"Unsigned Integers","text":"<p>With N bits, values range from: [ 0 \\text{ to } (2^N - 1) ]</p> Bits Range 8-bit 0 \u2192 255 16-bit 0 \u2192 65,535 32-bit 0 \u2192 ~4.29 billion"},{"location":"wk_two/#signed-integers-twos-complement","title":"Signed Integers (Two\u2019s Complement)","text":"<p>To negate a number:</p> <ol> <li>Invert each bit.</li> <li>Add 1.</li> </ol> Example: Represent \u221224\u2081\u2080 using two's complement <pre><code>   +24 = 00011000\u2082\ninvert \u2192 11100111\u2082\n    +1 \u2192 11101000\u2082\n         -------- \n   \u221224 = 11101000\u2082\n</code></pre>"},{"location":"wk_two/#twos-complement-integer-range","title":"Two\u2019s Complement Integer Range","text":"<p>Formula: \u2212(2\u207f\u207b\u00b9) \u2192 (2\u207f\u207b\u00b9 \u2212 1)</p> Bits Range 8-bit \u2212128   \u2192 +127 16-bit \u221232768 \u2192 +32767"},{"location":"wk_two/#subtraction-using-twos-complement","title":"Subtraction Using Two\u2019s Complement","text":"<p>Formula: a - b = a + (-b)</p> Example: 3 \u2212 13 using two's complement <pre><code>       3 = 00000011\u2082  \n\n     +13 = 00001101\u2082  \n  invert \u2192 11110010\u2082  \n      +1 \u2192 11110011\u2082  \n           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n     \u221213 = 11110011\u2082\n\n\n       3 = 00000011\u2082  \n + (\u221213) = 11110011\u2082\n           --------\n         = 11110110\u2082 \n\nSince the result begins with 1, it is negative.  \nTake the two\u2019s complement again to find its value:\n\n3 + (-13) = 11110110\u2082 \n   invert \u2192 00001001\u2082 \n       +1 \u2192 00001010\u2082 = \u221210\u2081\u2080\n</code></pre> <p>Bit width must be sufficient</p> <p>Two\u2019s complement arithmetic only works correctly when the chosen bit width can represent both operands and their result.  If the result falls outside the representable range (e.g. outside \u2212128 to +127 for 8-bit signed integers), overflow occurs and the computed value will be incorrect.</p>"},{"location":"wk_two/#floating-point-numbers-ieee-754-a-natural-analogy-to-scientific-notation","title":"Floating-Point Numbers (IEEE 754): A Natural Analogy to Scientific Notation","text":"<p>Computers represent real numbers using floating-point format, which works like scientific notation\u2014but in binary instead of decimal. This lets us efficiently store both very large and very small numbers using a fixed number of bits.</p>"},{"location":"wk_two/#1-scientific-notation-the-decimal-version","title":"1. Scientific Notation: The Decimal Version","text":"<p>In decimal, numbers can be written in scientific notation as:</p> <pre><code>(\u22121)\u02e2 \u00d7 m \u00d7 10\u1d31\n</code></pre> <p>where:</p> <ul> <li>S \u2013 sign (0 = positive, 1 = negative)  </li> <li>m \u2013 significand (or mantissa), with exactly one non-zero digit before the decimal point  </li> <li>E \u2013 exponent, the power of ten that scales the significand  </li> </ul> <p>Example</p> <pre><code>\u22125470 = (\u22121)\u00b9 \u00d7 5.47 \u00d7 10\u00b3\n</code></pre> <p>This is the normalized form, ensuring a single non-zero digit appears before the decimal point.</p>"},{"location":"wk_two/#2-binary-scientific-notation-same-idea-base-2","title":"2. Binary Scientific Notation: Same Idea, Base-2","text":"<p>The same idea applies in binary:</p> <pre><code>(\u22121)\u02e2 \u00d7 m \u00d7 2\u1d31\n</code></pre> <p>where:</p> <ul> <li>S \u2013 sign bit (0 = positive, 1 = negative)  </li> <li>m \u2013 significand, representing the significant binary digits  </li> <li>E \u2013 exponent, the power of two that scales the significand  </li> </ul> <p>In binary, the only possible non-zero digit is 1, so all normalized numbers have:</p> <pre><code>m = 1.F\n</code></pre> <p>where F is the fractional part after the binary point.</p> <p>The general form becomes:</p> <p><pre><code>(\u22121)\u02e2 \u00d7 (1.F) \u00d7 2\u1d31\n</code></pre> When storing these numbers, we can drop constants that never change. Therefore we only need to store:</p> <ul> <li>S \u2013 sign bit  </li> <li>E \u2013 exponent  </li> <li>F \u2013 fraction bits  </li> </ul>"},{"location":"wk_two/#3-the-exponent-bias","title":"3. The Exponent Bias","text":"<p>Exponents can be positive or negative, but hardware stores unsigned binary values.  To handle both, IEEE 754 adds a bias so all exponents fit in a positive range.</p> Format Exponent Bits Bias Formula Bias Value Single 8 2\u2077 \u2212 1 127 Double 11 2\u00b9\u2070 \u2212 1 1023 <p>The stored exponent is:</p> <pre><code>E\u1d66 = E + Bias\n</code></pre> <p>Example</p> <pre><code>If E = \u22121 (single precision)\nE\u1d66 = \u22121 + 127 = 126 = 01111110\u2082\n</code></pre>"},{"location":"wk_two/#4-ieee-754-bit-layout","title":"4. IEEE 754 Bit Layout","text":"<p>Each floating-point number is divided into three fields, each with a fixed bit length:</p> Precision (Format) Total Bits Sign Field (bits) Exponent Field (bits) Fraction Field (bits) Single (float) 32 1 8 23 Double (double) 64 1 11 52 Example \u2013 Encoding \u22120.75\u2081\u2080 as a 32-bit Float <p>Step 1 \u2013 Include the sign and convert to binary</p> <p>The number is negative, so the sign bit (S) will be 1.</p> <p>Convert the magnitude (0.75) to binary by repeatedly multiplying by 2 and recording each carry:</p> <pre><code>0.75 \u00d7 2 = 1.5 \u2192 1\n0.50 \u00d7 2 = 1.0 \u2192 1\n</code></pre> <p>So the positive part is:</p> <pre><code>0.75\u2081\u2080 = 0.11\u2082\n</code></pre> <p>Add the sign and express it in binary scientific notation:</p> <pre><code>\u22120.75\u2081\u2080 = (\u22121)\u00b9 \u00d7 1.1\u2082 \u00d7 2\u207b\u00b9\n</code></pre> <p>Step 2 \u2013 Identify the fields</p> Component Description Value S Sign bit 1 (negative) E Exponent \u22121 F Fraction 1 <p>Step 3 \u2013 Apply the bias</p> <pre><code>E\u1d66 = E + Bias = \u22121 + 127 = 126 = 01111110\u2082\n</code></pre> <p>Step 4 \u2013 Assemble the fields</p> Sign Exponent Fraction 1 01111110 10000000000000000000000 <p>*note that the F is padded with 0s to fill all 23 bits</p> <p>Step 5 \u2013 Final 32-bit representation</p> <pre><code>10111111010000000000000000000000\u2082\n</code></pre>"}]}